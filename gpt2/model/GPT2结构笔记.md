```python
    if head_mask is not None:
        attn_weights = attn_weights * head_mask
```


这两行代码实现的是**注意力头掩码机制**，用于按头(head)维度控制注意力权重的有效性。我们可以通过张量形状分析和广播机制来拆解其矩阵运算逻辑。

### 数学拆解
假设：
- `attn_weights.shape = (batch_size=2, num_heads=3, seq_len=4, seq_len=4)`
- `head_mask.shape = (num_heads=3,)` → 例如 `[0.5, 1.0, 0.0]`

运算过程等价于：
```python
# 将head_mask扩展为 (1, 3, 1, 1)
expanded_mask = head_mask.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)

# 广播乘法（逐头缩放）
attn_weights = attn_weights * expanded_mask
```

### 矩阵级示例
以第一个样本（batch=0）为例：

**原始注意力权重**（每个头独立）：
```
Head 0 (mask=0.5):       Head 1 (mask=1.0):       Head 2 (mask=0.0):
[[1 2 3 4],              [[5 6 7 8],              [[9 10 11 12],
 [5 6 7 8],               [1 2 3 4],               [5 6 7 8],
 [9 10 11 12],            [5 6 7 8],               [1 2 3 4],
 [13 14 15 16]]           [9 10 11 12]]            [5 6 7 8]]
```

**运算结果**：
```
Head 0 (×0.5):           Head 1 (×1.0):           Head 2 (×0.0):
[[0.5 1.0 1.5 2.0],      [[5 6 7 8],              [[0 0 0 0],
 [2.5 3.0 3.5 4.0],       [1 2 3 4],               [0 0 0 0],
 [4.5 5.0 5.5 6.0],       [5 6 7 8],               [0 0 0 0],
 [6.5 7.0 7.5 8.0]]       [9 10 11 12]]            [0 0 0 0]]
```

### 设计意图
1. **动态头控制**：通过掩码值：
   - `1.0` → 保留该头
   - `0.0` → 完全屏蔽（如剪枝）
   - `(0,1)` → 部分保留（如知识蒸馏）

2. **广播机制**：通过维度扩展实现：
   ```
   (3,) → (1,3,1,1) → 广播到 (2,3,4,4)
   ```

3. **计算效率**：比逐头循环更高效，利用GPU并行性一次性完成所有头的掩码操作。
